{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T18:53:51.647965Z",
     "start_time": "2019-11-25T18:53:51.639989Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.path.append('./keras-spp/')\n",
    "# from spp.SpatialPyramidPooling import SpatialPyramidPooling\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Dropout, Flatten, GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage.color import rgb2gray\n",
    "from keras.utils import np_utils\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from skimage.transform import resize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:03:58.143439Z",
     "start_time": "2019-11-25T19:03:50.451513Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "master_folder = '../../Desktop/PhotosDataset'\n",
    "\n",
    "imgs = []\n",
    "labels = []\n",
    "for photo in os.listdir(master_folder+'/Alex'):\n",
    "    img = imread(master_folder+'/Alex/'+photo)\n",
    "    imgs.append(resize(img,(200,200,3)))\n",
    "    labels.append('Alex')\n",
    "for photo in os.listdir(master_folder+'/Hunter'):\n",
    "    img = imread(master_folder+'/Hunter/'+photo)\n",
    "    imgs.append(resize(img,(200,200,3)))\n",
    "    labels.append('Hunter')\n",
    "labels = np.array([[1,0] if label is 'Hunter' else [0,1] for label in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Massage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:04:03.952920Z",
     "start_time": "2019-11-25T19:04:03.617848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n"
     ]
    }
   ],
   "source": [
    "# Take out alpha component of image\n",
    "#imgs = [img[:,:,[0,1,2]] for img in imgs]\n",
    "\n",
    "wavelet_imgs = []\n",
    "for i, img in enumerate(imgs):\n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "    coeffs2 = pywt.dwt2(img, 'bior1.3')\n",
    "    wavelet_imgs.append(coeffs2[1][:2])\n",
    "\n",
    "for i,stack in enumerate(wavelet_imgs):\n",
    "    stack = np.dstack(stack)\n",
    "    wavelet_imgs[i] = stack\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(wavelet_imgs, labels, test_size=.1)\n",
    "x_train = np.stack(x_train,axis=0)\n",
    "x_test = np.stack(x_test,axis=0)\n",
    "\n",
    "num_classes = 2\n",
    "# y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "# y_test = np_utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:04:12.209540Z",
     "start_time": "2019-11-25T19:04:12.013950Z"
    }
   },
   "outputs": [],
   "source": [
    "# x_train = np.stack(x_train,axis=0)\n",
    "# x_test = np.stack(x_test,axis=0)\n",
    "\n",
    "num_classes = 2\n",
    "# y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "# y_test = np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:04:14.158734Z",
     "start_time": "2019-11-25T19:04:13.882556Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Use a generator to feed data because data images are of different dimensions so a numpy array cant\n",
    "# # be constructed\n",
    "# def generator(x, y):\n",
    "#     while True:\n",
    "#         for i,img in enumerate(x):\n",
    "#             yield np.expand_dims(img,axis=0), np.expand_dims(y[i],axis=0)\n",
    "            \n",
    "\n",
    "            \n",
    "# gen = generator(x_train, y_train)\n",
    "# val_gen = generator(x_test,y_test)\n",
    "num_channels = 2\n",
    "# num_classes = 2\n",
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=.2,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=.2,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=True)  # randomly flip images\n",
    "\n",
    "datagen.fit(x_train)\n",
    "# gen = generator(x_train, y_train)\n",
    "# val_gen = generator(x_test,y_test)\n",
    "\n",
    "batch_size=4\n",
    "epochs=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(332, 2)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with no transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:06:53.384248Z",
     "start_time": "2019-11-25T19:04:15.752457Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras_preprocessing/image/numpy_array_iterator.py:127: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (332, 200, 102, 8) (8 channels).\n",
      "  str(self.x.shape[channels_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "83/83 [==============================] - 7s 81ms/step - loss: 0.6999 - acc: 0.5753 - val_loss: 0.6651 - val_acc: 0.8649\n",
      "Epoch 2/10\n",
      "83/83 [==============================] - 6s 76ms/step - loss: 0.2230 - acc: 0.9337 - val_loss: 0.4876 - val_acc: 0.9189\n",
      "Epoch 3/10\n",
      "83/83 [==============================] - 7s 80ms/step - loss: 0.0529 - acc: 0.9849 - val_loss: 0.5049 - val_acc: 0.8649\n",
      "Epoch 4/10\n",
      "83/83 [==============================] - 7s 78ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.4672 - val_acc: 0.8649\n",
      "Epoch 5/10\n",
      "83/83 [==============================] - 6s 77ms/step - loss: 3.4346e-04 - acc: 1.0000 - val_loss: 0.4423 - val_acc: 0.8649\n",
      "Epoch 6/10\n",
      "83/83 [==============================] - 6s 77ms/step - loss: 2.4497e-04 - acc: 1.0000 - val_loss: 0.4251 - val_acc: 0.8649\n",
      "Epoch 7/10\n",
      "83/83 [==============================] - 7s 81ms/step - loss: 0.0920 - acc: 0.9699 - val_loss: 0.5231 - val_acc: 0.8649\n",
      "Epoch 8/10\n",
      "83/83 [==============================] - 7s 81ms/step - loss: 7.7236e-05 - acc: 1.0000 - val_loss: 0.4682 - val_acc: 0.8649\n",
      "Epoch 9/10\n",
      "83/83 [==============================] - 7s 83ms/step - loss: 7.1929e-05 - acc: 1.0000 - val_loss: 0.4123 - val_acc: 0.8649\n",
      "Epoch 10/10\n",
      "83/83 [==============================] - 7s 80ms/step - loss: 9.1453e-06 - acc: 1.0000 - val_loss: 0.4005 - val_acc: 0.8649\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Note that we leave the image size as None to allow multiple image sizes\n",
    "model.add(Conv2D(96, (7, 7),strides=2, padding='same',\n",
    "                 activation='relu',input_shape=(x_train.shape[1],x_train.shape[2],x_train.shape[3]),\n",
    "                name='input_layer'))\n",
    "model.add(MaxPooling2D((3,3)))\n",
    "\n",
    "model.add(Conv2D(48, (5, 5),strides=1, padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D((3,3)))\n",
    "\n",
    "model.add(Conv2D(24, (3, 3),strides=1, padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Conv2D(24, (3, 3),strides=1, padding='same',activation='relu'))\n",
    "model.add(Flatten())\n",
    "# # Spatial Pooling layer to deal with differing image sizes\n",
    "# model.add(SpatialPyramidPooling([6, 3, 2,1]))\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "\n",
    "# Classification layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                   patience=15, verbose=1, mode='auto',\n",
    "                                   min_delta=0.0001, cooldown=0, min_lr=0))\n",
    "# opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "hist = model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                 batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),verbose=1,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilize transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:10:41.602441Z",
     "start_time": "2019-11-25T19:10:39.456789Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1130 18:03:41.065224 140734961169856 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/saving.py:327: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "filepath = \"wavelet_model.h5\"\n",
    "trans_model = load_model(filepath)\n",
    "\n",
    "# Allow no weight adjustments for the pretrained layers\n",
    "for layer in trans_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "trans_model.add(Conv2D(32,(2,2)))\n",
    "trans_model.add(MaxPooling2D((1,1)))\n",
    "trans_model.add(Flatten())\n",
    "trans_model.add(Dense(128,activation='relu'))\n",
    "\n",
    "# Classification Layer\n",
    "trans_model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:11:28.339449Z",
     "start_time": "2019-11-25T19:10:43.133805Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras_preprocessing/image/numpy_array_iterator.py:127: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (332, 200, 102, 8) (8 channels).\n",
      "  str(self.x.shape[channels_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "83/83 [==============================] - 9s 103ms/step - loss: 7.9252 - acc: 0.5030 - val_loss: 10.4550 - val_acc: 0.3514\n",
      "Epoch 2/10\n",
      "83/83 [==============================] - 8s 91ms/step - loss: 8.0105 - acc: 0.5030 - val_loss: 10.4550 - val_acc: 0.3514\n",
      "Epoch 3/10\n",
      "83/83 [==============================] - 7s 90ms/step - loss: 8.0105 - acc: 0.5030 - val_loss: 10.4550 - val_acc: 0.3514\n",
      "Epoch 4/10\n",
      "83/83 [==============================] - 8s 92ms/step - loss: 8.0105 - acc: 0.5030 - val_loss: 10.4550 - val_acc: 0.3514\n",
      "Epoch 5/10\n",
      "83/83 [==============================] - 8s 98ms/step - loss: 8.0105 - acc: 0.5030 - val_loss: 10.4550 - val_acc: 0.3514\n",
      "Epoch 6/10\n",
      "83/83 [==============================] - 9s 102ms/step - loss: 8.0105 - acc: 0.5030 - val_loss: 10.4550 - val_acc: 0.3514\n",
      "Epoch 7/10\n",
      "83/83 [==============================] - 8s 96ms/step - loss: 8.0105 - acc: 0.5030 - val_loss: 10.4550 - val_acc: 0.3514\n",
      "Epoch 8/10\n",
      "83/83 [==============================] - 9s 107ms/step - loss: 8.0105 - acc: 0.5030 - val_loss: 10.4550 - val_acc: 0.3514\n",
      "Epoch 9/10\n",
      "83/83 [==============================] - 8s 97ms/step - loss: 8.0105 - acc: 0.5030 - val_loss: 10.4550 - val_acc: 0.3514\n",
      "Epoch 10/10\n",
      "83/83 [==============================] - 8s 100ms/step - loss: 8.0105 - acc: 0.5030 - val_loss: 10.4550 - val_acc: 0.3514\n"
     ]
    }
   ],
   "source": [
    "callbacks = []\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                   patience=15, verbose=1, mode='auto',\n",
    "                                   min_delta=0.0001, cooldown=0, min_lr=0))\n",
    "opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "trans_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "# Fit model\n",
    "trans_hist = trans_model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                 batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),verbose=1,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T16:27:01.888663Z",
     "start_time": "2019-11-25T16:12:50.812Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_pred = []\n",
    "reg_pred= []\n",
    "for img in x_test:\n",
    "    trans_pred.append(trans_model.predict(np.expand_dims(img, axis=0)))\n",
    "    reg_pred.append(model.predict(np.expand_dims(img, axis=0)))\n",
    "\n",
    "reg_pred = [\"Hunter\" if x[0][0] > x[0][1] else \"Alex\" for x in reg_pred]\n",
    "trans_pred = [\"Hunter\" if x[0][0] > x[0][1] else \"Alex\" for x in trans_pred]\n",
    "\n",
    "y_test = [\"Hunter\" if x[0] > x[1] else \"Alex\" for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T16:27:01.893654Z",
     "start_time": "2019-11-25T16:12:51.896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with No transfer learning\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Alex       0.72      1.00      0.84        13\n",
      "      Hunter       1.00      0.79      0.88        24\n",
      "\n",
      "    accuracy                           0.86        37\n",
      "   macro avg       0.86      0.90      0.86        37\n",
      "weighted avg       0.90      0.86      0.87        37\n",
      "\n",
      "Model with transfer learning\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Model with No transfer learning\")\n",
    "print(classification_report(y_test, reg_pred))\n",
    "print(\"Model with transfer learning\")\n",
    "#print(classification_report(y_test, trans_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "500.573px",
    "left": "1001.34px",
    "right": "20px",
    "top": "126.994px",
    "width": "349.983px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
