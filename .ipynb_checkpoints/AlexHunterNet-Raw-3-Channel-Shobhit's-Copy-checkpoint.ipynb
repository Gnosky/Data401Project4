{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T18:53:51.647965Z",
     "start_time": "2019-11-25T18:53:51.639989Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.path.append('./keras-spp/')\n",
    "# from spp.SpatialPyramidPooling import SpatialPyramidPooling\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Dropout, Flatten, GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage.color import rgb2gray\n",
    "from keras.utils import np_utils\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from skimage.transform import resize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:03:58.143439Z",
     "start_time": "2019-11-25T19:03:50.451513Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "master_folder = '../../Desktop/PhotosDataset'\n",
    "\n",
    "imgs = []\n",
    "labels = []\n",
    "for photo in os.listdir(master_folder+'/Alex'):\n",
    "    img = imread(master_folder+'/Alex/'+photo)\n",
    "    imgs.append(resize(img,(200,200,3)))\n",
    "    labels.append('Alex')\n",
    "for photo in os.listdir(master_folder+'/Hunter'):\n",
    "    img = imread(master_folder+'/Hunter/'+photo)\n",
    "    imgs.append(resize(img,(200,200,3)))\n",
    "    labels.append('Hunter')\n",
    "labels = np.array([label == 'Hunter' for label in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Massage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:04:03.952920Z",
     "start_time": "2019-11-25T19:04:03.617848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n"
     ]
    }
   ],
   "source": [
    "# Take out alpha component of image\n",
    "#imgs = [img[:,:,[0,1,2]] for img in imgs]\n",
    "\n",
    "wavelet_imgs = []\n",
    "for i, img in enumerate(imgs):\n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "    coeffs2 = pywt.dwt2(img, 'bior1.3')\n",
    "    wavelet_imgs.append(coeffs2[1][:2])\n",
    "\n",
    "for i,stack in enumerate(wavelet_imgs):\n",
    "    stack = np.dstack(stack)\n",
    "    wavelet_imgs[i] = stack\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(wavelet_imgs, labels, test_size=.1)\n",
    "x_train = np.stack(x_train,axis=0)\n",
    "x_test = np.stack(x_test,axis=0)\n",
    "\n",
    "num_classes = 102\n",
    "# y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "# y_test = np_utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:04:12.209540Z",
     "start_time": "2019-11-25T19:04:12.013950Z"
    }
   },
   "outputs": [],
   "source": [
    "# x_train = np.stack(x_train,axis=0)\n",
    "# x_test = np.stack(x_test,axis=0)\n",
    "\n",
    "num_classes = 2\n",
    "# y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "# y_test = np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:04:14.158734Z",
     "start_time": "2019-11-25T19:04:13.882556Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras_preprocessing/image/image_data_generator.py:940: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (332, 200, 102, 8) (8 channels).\n",
      "  ' channels).')\n"
     ]
    }
   ],
   "source": [
    "# # Use a generator to feed data because data images are of different dimensions so a numpy array cant\n",
    "# # be constructed\n",
    "# def generator(x, y):\n",
    "#     while True:\n",
    "#         for i,img in enumerate(x):\n",
    "#             yield np.expand_dims(img,axis=0), np.expand_dims(y[i],axis=0)\n",
    "            \n",
    "\n",
    "            \n",
    "# gen = generator(x_train, y_train)\n",
    "# val_gen = generator(x_test,y_test)\n",
    "num_channels = 2\n",
    "# num_classes = 2\n",
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=.2,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=.2,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=True)  # randomly flip images\n",
    "\n",
    "datagen.fit(x_train)\n",
    "# gen = generator(x_train, y_train)\n",
    "# val_gen = generator(x_test,y_test)\n",
    "\n",
    "batch_size=4\n",
    "epochs=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9279e220d9b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with no transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:06:53.384248Z",
     "start_time": "2019-11-25T19:04:15.752457Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1130 17:56:42.477987 140734961169856 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1130 17:56:42.511408 140734961169856 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1130 17:56:42.517097 140734961169856 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1130 17:56:42.548025 140734961169856 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1130 17:56:42.631120 140734961169856 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1130 17:56:42.663361 140734961169856 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras_preprocessing/image/numpy_array_iterator.py:127: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (332, 200, 102, 8) (8 channels).\n",
      "  str(self.x.shape[channels_axis]) + ' channels).')\n",
      "W1130 17:56:42.940948 140734961169856 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1130 17:56:43.029519 140734961169856 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_2 to have shape (2,) but got array with shape (102,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0cb4fb6ceea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                     callbacks=callbacks)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    142\u001b[0m                                      str(validation_data))\n\u001b[1;32m    143\u001b[0m                 val_x, val_y, val_sample_weights = model._standardize_user_data(\n\u001b[0;32m--> 144\u001b[0;31m                     val_x, val_y, val_sample_weight)\n\u001b[0m\u001b[1;32m    145\u001b[0m                 \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 if model.uses_learning_phase and not isinstance(K.learning_phase(),\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_2 to have shape (2,) but got array with shape (102,)"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Note that we leave the image size as None to allow multiple image sizes\n",
    "model.add(Conv2D(96, (7, 7),strides=2, padding='same',\n",
    "                 activation='relu',input_shape=(x_train.shape[1],x_train.shape[2],x_train.shape[3]),\n",
    "                name='input_layer'))\n",
    "model.add(MaxPooling2D((3,3)))\n",
    "\n",
    "model.add(Conv2D(48, (5, 5),strides=1, padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D((3,3)))\n",
    "\n",
    "model.add(Conv2D(24, (3, 3),strides=1, padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Conv2D(24, (3, 3),strides=1, padding='same',activation='relu'))\n",
    "model.add(Flatten())\n",
    "# # Spatial Pooling layer to deal with differing image sizes\n",
    "# model.add(SpatialPyramidPooling([6, 3, 2,1]))\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "\n",
    "# Classification layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                   patience=15, verbose=1, mode='auto',\n",
    "                                   min_delta=0.0001, cooldown=0, min_lr=0))\n",
    "# opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "hist = model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                 batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),verbose=1,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilize transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:10:41.602441Z",
     "start_time": "2019-11-25T19:10:39.456789Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath = \"wavelet_model.h5\"\n",
    "trans_model = load_model(filepath)\n",
    "\n",
    "# Allow no weight adjustments for the pretrained layers\n",
    "for layer in trans_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "trans_model.add(Conv2D(32,(2,2)))\n",
    "trans_model.add(MaxPooling2D((1,1)))\n",
    "trans_model.add(Flatten())\n",
    "trans_model.add(Dense(128,activation='relu'))\n",
    "\n",
    "# Classification Layer\n",
    "trans_model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:11:28.339449Z",
     "start_time": "2019-11-25T19:10:43.133805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "46/46 [==============================] - 9s 201ms/step - loss: 0.7740 - acc: 0.6196 - val_loss: 0.7140 - val_acc: 0.4865\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 8s 176ms/step - loss: 0.7605 - acc: 0.5870 - val_loss: 0.7197 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 8s 180ms/step - loss: 0.6440 - acc: 0.6957 - val_loss: 0.7189 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 9s 202ms/step - loss: 0.6670 - acc: 0.6522 - val_loss: 0.7349 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 11s 231ms/step - loss: 0.6403 - acc: 0.6413 - val_loss: 0.7471 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 9s 189ms/step - loss: 0.6399 - acc: 0.6522 - val_loss: 0.7558 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 9s 186ms/step - loss: 0.6554 - acc: 0.7011 - val_loss: 0.7707 - val_acc: 0.4865\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 9s 186ms/step - loss: 0.5858 - acc: 0.6957 - val_loss: 0.7998 - val_acc: 0.4865\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 8s 180ms/step - loss: 0.5973 - acc: 0.7065 - val_loss: 0.8318 - val_acc: 0.4865\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 9s 188ms/step - loss: 0.6370 - acc: 0.7174 - val_loss: 0.8673 - val_acc: 0.4865\n"
     ]
    }
   ],
   "source": [
    "callbacks = []\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                   patience=15, verbose=1, mode='auto',\n",
    "                                   min_delta=0.0001, cooldown=0, min_lr=0))\n",
    "opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "trans_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "# Fit model\n",
    "trans_hist = trans_model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                 batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),verbose=1,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T16:27:01.888663Z",
     "start_time": "2019-11-25T16:12:50.812Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_pred = []\n",
    "reg_pred= []\n",
    "for img in x_test:\n",
    "    trans_pred.append(trans_model.predict(np.expand_dims(img, axis=0)))\n",
    "    reg_pred.append(model.predict(np.expand_dims(img, axis=0)))\n",
    "\n",
    "reg_pred = [\"Hunter\" if x[0][0] > x[0][1] else \"Alex\" for x in reg_pred]\n",
    "trans_pred = [\"Hunter\" if x[0][0] > x[0][1] else \"Alex\" for x in trans_pred]\n",
    "\n",
    "y_test = [\"Hunter\" if x[0] > x[1] else \"Alex\" for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T16:27:01.893654Z",
     "start_time": "2019-11-25T16:12:51.896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with No transfer learning\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Alex       0.49      1.00      0.65        90\n",
      "      Hunter       0.00      0.00      0.00        95\n",
      "\n",
      "    accuracy                           0.49       185\n",
      "   macro avg       0.24      0.50      0.33       185\n",
      "weighted avg       0.24      0.49      0.32       185\n",
      "\n",
      "Model with transfer learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Model with No transfer learning\")\n",
    "print(classification_report(y_test, reg_pred))\n",
    "print(\"Model with transfer learning\")\n",
    "#print(classification_report(y_test, trans_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "500.573px",
    "left": "1001.34px",
    "right": "20px",
    "top": "126.994px",
    "width": "349.983px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
