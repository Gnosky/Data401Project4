{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T20:10:05.941381Z",
     "start_time": "2019-11-29T20:10:05.929396Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('./keras-spp/')\n",
    "# from spp.SpatialPyramidPooling import SpatialPyramidPooling\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, BatchNormalization, GaussianNoise\n",
    "from keras.layers import Dropout, Flatten, GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage.color import rgb2gray\n",
    "from keras.utils import np_utils\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from skimage.transform import resize\n",
    "\n",
    "np.random.seed(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T20:10:16.693970Z",
     "start_time": "2019-11-29T20:10:07.460906Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gjber\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n",
      "c:\\users\\gjber\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\transform\\_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "c:\\users\\gjber\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\transform\\_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "c:\\users\\gjber\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "master_folder = '../GrantData401Project4/PhotosDataset/'\n",
    "\n",
    "imgs = []\n",
    "labels = []\n",
    "for photo in os.listdir(master_folder+'/Alex'):\n",
    "    img = imread(master_folder+'/Alex/'+photo)\n",
    "    imgs.append(resize(img,(200,200,4)))\n",
    "    labels.append('Alex')\n",
    "for photo in os.listdir(master_folder+'/Hunter'):\n",
    "    img = imread(master_folder+'/Hunter/'+photo)\n",
    "    imgs.append(resize(img,(200,200,4)))\n",
    "    labels.append('Hunter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Massage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T20:11:10.999596Z",
     "start_time": "2019-11-29T20:11:10.701298Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take out alpha component of image\n",
    "imgs = [img[:,:,[0,1,2]] for img in imgs]\n",
    "\n",
    "labels = np.array([[1,0] if label is 'Hunter' else [0,1] for label in labels])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(imgs, labels, test_size=.7,\n",
    "                                                    stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T20:11:12.608931Z",
     "start_time": "2019-11-29T20:11:12.291596Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = np.stack(x_train,axis=0)\n",
    "x_test = np.stack(x_test,axis=0)\n",
    "\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T20:11:13.344915Z",
     "start_time": "2019-11-29T20:11:13.220162Z"
    }
   },
   "outputs": [],
   "source": [
    "num_channels = 3\n",
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=.2,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=.2,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "batch_size=4\n",
    "epochs=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with no transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T20:12:00.249484Z",
     "start_time": "2019-11-29T20:11:14.423166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "27/27 [==============================] - 9s 333ms/step - loss: 0.0214 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "27/27 [==============================] - 10s 357ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "27/27 [==============================] - 9s 331ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "27/27 [==============================] - 8s 310ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "27/27 [==============================] - 8s 314ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Note that we leave the image size as None to allow multiple image sizes\n",
    "model.add(Conv2D(48, (7, 7),strides=2, padding='same',\n",
    "                 activation='relu',input_shape=(200, 200,num_channels),\n",
    "                name='input_layer'))\n",
    "model.add(MaxPooling2D((3,3)))\n",
    "model.add(GaussianNoise(.1))\n",
    "\n",
    "model.add(Conv2D(24, (3, 3),strides=1, padding='same',activation='relu'))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(.3))\n",
    "\n",
    "# Classification layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                   patience=15, verbose=1, mode='auto',\n",
    "                                   min_delta=0.0001, cooldown=0, min_lr=0))\n",
    "# opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "hist = model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                 batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),verbose=1,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilize transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T20:12:03.310208Z",
     "start_time": "2019-11-29T20:12:01.387691Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gjber\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:327: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "filepath = \"caltech_101_raw_3_channel_no_pyramid.h5\"\n",
    "trans_model = load_model(filepath)\n",
    "\n",
    "# Allow no weight adjustments for the pretrained layers\n",
    "for layer in trans_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "trans_model.add(Conv2D(32,(2,2)))\n",
    "trans_model.add(MaxPooling2D((1,1)))\n",
    "trans_model.add(Flatten())\n",
    "trans_model.add(Dense(128,activation='relu'))\n",
    "\n",
    "# Classification Layer\n",
    "trans_model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T20:13:58.897332Z",
     "start_time": "2019-11-29T20:12:10.312501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "27/27 [==============================] - 21s 786ms/step - loss: 0.3242 - acc: 1.0000 - val_loss: 0.2581 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "27/27 [==============================] - 21s 761ms/step - loss: 0.0532 - acc: 1.0000 - val_loss: 0.0700 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "27/27 [==============================] - 21s 783ms/step - loss: 0.0091 - acc: 1.0000 - val_loss: 0.0323 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "27/27 [==============================] - 24s 888ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.0198 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "27/27 [==============================] - 21s 783ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0137 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "callbacks = []\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                   patience=15, verbose=1, mode='auto',\n",
    "                                   min_delta=0.0001, cooldown=0, min_lr=0))\n",
    "opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "trans_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "# Fit model\n",
    "trans_hist = trans_model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                 batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),verbose=1,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T20:15:19.671919Z",
     "start_time": "2019-11-29T20:15:01.194695Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_pred = []\n",
    "reg_pred= []\n",
    "for img in x_test:\n",
    "    trans_pred.append(trans_model.predict(np.expand_dims(img, axis=0)))\n",
    "    reg_pred.append(model.predict(np.expand_dims(img, axis=0)))\n",
    "\n",
    "reg_pred = [\"Hunter\" if x[0][0] > x[0][1] else \"Alex\" for x in reg_pred]\n",
    "trans_pred = [\"Hunter\" if x[0][0] > x[0][1] else \"Alex\" for x in trans_pred]\n",
    "\n",
    "y_test = [\"Hunter\" if x[0] > x[1] else \"Alex\" for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T20:15:20.208319Z",
     "start_time": "2019-11-29T20:15:20.190547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with No transfer learning\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Alex       1.00      1.00      1.00       259\n",
      "\n",
      "avg / total       1.00      1.00      1.00       259\n",
      "\n",
      "Model with transfer learning\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Alex       1.00      1.00      1.00       259\n",
      "\n",
      "avg / total       1.00      1.00      1.00       259\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Model with No transfer learning\")\n",
    "print(classification_report(y_test, reg_pred))\n",
    "print(\"Model with transfer learning\")\n",
    "print(classification_report(y_test, trans_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "500.573px",
    "left": "1001.34px",
    "right": "20px",
    "top": "126.994px",
    "width": "349.983px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
