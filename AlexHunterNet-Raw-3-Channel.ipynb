{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T18:53:51.647965Z",
     "start_time": "2019-11-25T18:53:51.639989Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('./keras-spp/')\n",
    "# from spp.SpatialPyramidPooling import SpatialPyramidPooling\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Dropout, Flatten, GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage.color import rgb2gray\n",
    "from keras.utils import np_utils\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from skimage.transform import resize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:03:58.143439Z",
     "start_time": "2019-11-25T19:03:50.451513Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gjber\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n",
      "c:\\users\\gjber\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\transform\\_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "c:\\users\\gjber\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\transform\\_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "c:\\users\\gjber\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "master_folder = '../GrantData401Project4/PhotosDataset/'\n",
    "\n",
    "imgs = []\n",
    "labels = []\n",
    "for photo in os.listdir(master_folder+'/Alex'):\n",
    "    img = imread(master_folder+'/Alex/'+photo)\n",
    "    imgs.append(resize(img,(200,200,4)))\n",
    "    labels.append('Alex')\n",
    "for photo in os.listdir(master_folder+'/Hunter'):\n",
    "    img = imread(master_folder+'/Hunter/'+photo)\n",
    "    imgs.append(resize(img,(200,200,4)))\n",
    "    labels.append('Hunter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Massage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:04:03.952920Z",
     "start_time": "2019-11-25T19:04:03.617848Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take out alpha component of image\n",
    "imgs = [img[:,:,[0,1,2]] for img in imgs]\n",
    "\n",
    "labels = np.array([[1,0] if label is 'Hunter' else [0,1] for label in labels])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(imgs, labels, test_size=.5,\n",
    "                                                    stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:04:12.209540Z",
     "start_time": "2019-11-25T19:04:12.013950Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = np.stack(x_train,axis=0)\n",
    "x_test = np.stack(x_test,axis=0)\n",
    "\n",
    "num_classes = 2\n",
    "# y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "# y_test = np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:04:14.158734Z",
     "start_time": "2019-11-25T19:04:13.882556Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Use a generator to feed data because data images are of different dimensions so a numpy array cant\n",
    "# # be constructed\n",
    "# def generator(x, y):\n",
    "#     while True:\n",
    "#         for i,img in enumerate(x):\n",
    "#             yield np.expand_dims(img,axis=0), np.expand_dims(y[i],axis=0)\n",
    "            \n",
    "\n",
    "            \n",
    "# gen = generator(x_train, y_train)\n",
    "# val_gen = generator(x_test,y_test)\n",
    "# num_channels = 3\n",
    "# num_classes = 2\n",
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=.2,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=.2,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=True)  # randomly flip images\n",
    "\n",
    "datagen.fit(x_train)\n",
    "# gen = generator(x_train, y_train)\n",
    "# val_gen = generator(x_test,y_test)\n",
    "\n",
    "batch_size=4\n",
    "epochs=10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with no transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:06:53.384248Z",
     "start_time": "2019-11-25T19:04:15.752457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "46/46 [==============================] - 17s 360ms/step - loss: 0.0498 - acc: 0.9783 - val_loss: 0.3321 - val_acc: 0.9189\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 19s 418ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.0599 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 16s 353ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.0591 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 15s 325ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.0591 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 15s 322ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.0591 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 15s 329ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.0591 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 15s 324ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.0591 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 15s 325ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.0591 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 15s 324ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.0591 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 15s 328ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.0591 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Note that we leave the image size as None to allow multiple image sizes\n",
    "model.add(Conv2D(96, (7, 7),strides=2, padding='same',\n",
    "                 activation='relu',input_shape=(200, 200,num_channels),\n",
    "                name='input_layer'))\n",
    "model.add(MaxPooling2D((3,3)))\n",
    "\n",
    "model.add(Conv2D(48, (5, 5),strides=1, padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D((3,3)))\n",
    "\n",
    "model.add(Conv2D(24, (3, 3),strides=1, padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Conv2D(24, (3, 3),strides=1, padding='same',activation='relu'))\n",
    "model.add(Flatten())\n",
    "# # Spatial Pooling layer to deal with differing image sizes\n",
    "# model.add(SpatialPyramidPooling([6, 3, 2,1]))\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "\n",
    "# Classification layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                   patience=15, verbose=1, mode='auto',\n",
    "                                   min_delta=0.0001, cooldown=0, min_lr=0))\n",
    "# opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "hist = model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                 batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),verbose=1,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilize transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:10:41.602441Z",
     "start_time": "2019-11-25T19:10:39.456789Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gjber\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:327: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "filepath = \"caltech_101_raw_3_channel_no_pyramid.h5\"\n",
    "trans_model = load_model(filepath)\n",
    "\n",
    "# Allow no weight adjustments for the pretrained layers\n",
    "for layer in trans_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "    \n",
    "# Spatial Pooling layer to deal with differing image sizes\n",
    "# trans_model.add(SpatialPyramidPooling([1, 2, 4]))\n",
    "trans_model.add(Conv2D(32,(3,3)))\n",
    "trans_model.add(MaxPooling2D((1,1)))\n",
    "trans_model.add(Flatten())\n",
    "trans_model.add(Dense(128,activation='relu'))\n",
    "# Classification Layer\n",
    "trans_model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T19:11:28.339449Z",
     "start_time": "2019-11-25T19:10:43.133805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "46/46 [==============================] - 9s 201ms/step - loss: 1.1073 - acc: 0.6848 - val_loss: 0.7385 - val_acc: 0.1622\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 8s 163ms/step - loss: 0.7023 - acc: 0.7880 - val_loss: 0.7603 - val_acc: 0.4865\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 8s 165ms/step - loss: 0.5840 - acc: 0.7826 - val_loss: 0.7906 - val_acc: 0.4865\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 8s 165ms/step - loss: 0.5051 - acc: 0.8152 - val_loss: 0.7999 - val_acc: 0.4865\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 9s 185ms/step - loss: 0.5345 - acc: 0.8098 - val_loss: 0.7399 - val_acc: 0.4865\n",
      "Epoch 6/10\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 0.4493 - acc: 0.7763"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-056e88fce36c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                     callbacks=callbacks)\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "callbacks = []\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                   patience=15, verbose=1, mode='auto',\n",
    "                                   min_delta=0.0001, cooldown=0, min_lr=0))\n",
    "opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "trans_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "# Fit model\n",
    "trans_hist = trans_model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                 batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),verbose=1,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T16:27:01.888663Z",
     "start_time": "2019-11-25T16:12:50.812Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_pred = []\n",
    "reg_pred= []\n",
    "for img in x_test:\n",
    "    trans_pred.append(trans_model.predict(np.expand_dims(img, axis=0)))\n",
    "    reg_pred.append(model.predict(np.expand_dims(img, axis=0)))\n",
    "\n",
    "reg_pred = [\"Hunter\" if x[0][0] > x[0][1] else \"Alex\" for x in reg_pred]\n",
    "trans_pred = [\"Hunter\" if x[0][0] > x[0][1] else \"Alex\" for x in trans_pred]\n",
    "\n",
    "y_test = [\"Hunter\" if x[0] > x[1] else \"Alex\" for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T16:27:01.893654Z",
     "start_time": "2019-11-25T16:12:51.896Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Model with No transfer learning\")\n",
    "print(classification_report(y_test, reg_pred))\n",
    "print(\"Model with transfer learning\")\n",
    "print(classification_report(y_test, trans_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "500.573px",
    "left": "1001.34px",
    "right": "20px",
    "top": "126.994px",
    "width": "349.983px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
